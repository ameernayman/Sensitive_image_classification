{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameernayman/Sensitive_image_classification/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM59X-o79RgC"
      },
      "source": [
        "# Text Classification to detect sensitive data exposure\n",
        "\n",
        "## Importing libraries and downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "8KD86wyWoQ0v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leaIgZqWI9HX"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "text_folder = \"/content/Sensitive_image_classification\"\n",
        "if os.path.exists(text_folder) == False:\n",
        "  !git clone https://github.com/ameernayman/Sensitive_image_classification.git\n",
        "\n",
        "dataset_text = text_folder + \"/dataset_text/\"\n",
        "sensitive_json = \"data_sensitive.json\"\n",
        "nonsensitive_json = \"data_nonsensitive.json\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_o7FrN_Jgr1"
      },
      "source": [
        "\n",
        "size_vocab = 3000\n",
        "dimensions = 32\n",
        "length_text = 60\n",
        "type_trunc='post'\n",
        "padding='post'\n",
        "out_of_vocb = \"<OOV>\"\n",
        "size_training = 25000"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i3YvPoe9ZaT"
      },
      "source": [
        "## Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIUEAZOfJyy-",
        "outputId": "cb4817b6-e8c0-4269-ba68-ebfefd649e46"
      },
      "source": [
        "textData = []\n",
        "textSentences = []\n",
        "textLabels = []\n",
        "\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "def load_data(filename):\n",
        "  with open(dataset_text + filename, 'r') as f:\n",
        "      data_store = json.load(f)\n",
        "  for value in data_store:\n",
        "    textSentences = value['data']\n",
        "    textLabels = value['is_sensitive']\n",
        "    for wrd in stopwords: \n",
        "      token = \" \" + wrd + \" \"\n",
        "      textSentences = textSentences.replace(token, \" \")\n",
        "    textData.append([textSentences, textLabels])\n",
        "\n",
        "\n",
        "load_data(sensitive_json)\n",
        "load_data(nonsensitive_json)\n",
        "\n",
        "random.shuffle(textData)\n",
        "\n",
        "\n",
        "for item in textData:\n",
        "  textSentences.append(item[0])\n",
        "  textLabels.append(item[1])\n",
        "\n",
        "\n",
        "training_snt_text= textSentences[0:size_training]\n",
        "validation_snt_text = textSentences[size_training:]\n",
        "training_lbls_text = textLabels[0:size_training]\n",
        "validation_lbls_text = textLabels[size_training:]\n",
        "\n",
        "print(\"Size of Training Data set is: \", len(training_snt_text))\n",
        "print(\"Training Data Sample:\", training_snt_text[0])\n",
        "print(\"Size of Validation Data set is: \", len(validation_snt_text))\n",
        "print(\"Validation Sample:\", validation_snt_text[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Training Data set is:  25000\n",
            "Training Data Sample: A convicted car thief diehard Chicago Cubs fan, Jimmy Dworski (Belushi) wins tickets World Series. Unfortunately, still couple days left serve prison warden (Héctor Elizondo) will not let leave come back. With help inmates, Jimmy stages riot can sneak prison see game. On way, finds Filofax uptight spineless advertising executive Spencer Barnes (Grodin), promises reward found.\n",
            "Over next day, Jimmy takes Barnes' identity—staying Malibu beach house Spencer's boss, flirting boss's daughter, even taking meeting powerful Japanese food company magnate named Sakamoto (Mako Iwamatsu). The fake \"Spencer\"'s unorthodox methods, beating magnate tennis telling poor quality food products, gets attention taken aback Sakamoto. However unconventional negotiations food company insult executives, seemingly ruining Spencer's reputation. Meanwhile, lacking precious Filofax, real Spencer Barnes spiraling gutter. Losing clothes, car money, rely old high school flame, neurotic overbearing Debbie Lipton (Anne De Salvo) keeps trying rekindle relationship him.\n",
            "Finally Jimmy Spencer come together meeting advertising executives, Spencer sacked boss. As consolation Jimmy takes Spencer World Series, Jimmy makes spectacular catch home-run ball hit Mark Grace, makes cameo. When security goes Jimmy, spotted Jumbotron, escape using Spencer's Filofax slide support wire stadium. Spencer patches marriage wife, become exasperated overworking. Jimmy sneaks back prison Spencer's help, serves last couple hours released, find Spencer waiting pick up. With promise beautiful girlfriend well-paying job advertising working Spencer, Jimmy's future looks bright, beloved Cubs, won World Series.\n",
            "Size of Validation Data set is:  6576\n",
            "Validation Sample: The setting time Greco-Turkish War (1919–1922). Two American soldiers fortune - Adam (Curtis) Josh (Bronson) - team 1922 Turkey protect three daughters Turkish governor journey across land secure Adam's boat. Along way, find may misled also thwart Turkish army colonel's attempt take hidden cache priceless treasure them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt01OaiEMbQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f14c2e-95c5-4ba9-df24-71867efb6b5b"
      },
      "source": [
        "\n",
        "tknizer = Tokenizer(num_words=size_vocab, oov_token=out_of_vocb)\n",
        "\n",
        "tknizer.fit_on_texts(training_snt_text)\n",
        "\n",
        "index_words = tknizer.word_index\n",
        "print(\"Size of word index:\", len(index_words))\n",
        "\n",
        "with open(\"word_index.json\", \"w\") as outfile:  \n",
        "    json.dump(index_words, outfile)\n",
        "    print(\"Saving the word index as JSON\")\n",
        "\n",
        "\n",
        "training_squnce = tknizer.texts_to_sequences(training_snt_text)\n",
        "padding_Tdataset = pad_sequences(training_squnce, maxlen=length_text, padding=padding, truncating=type_trunc)\n",
        "\n",
        "# Apply the same for validation data\n",
        "validation_squnce = tknizer.texts_to_sequences(validation_snt_text)\n",
        "padding_Vdataset = pad_sequences(validation_squnce, maxlen=length_text, padding=padding, truncating=type_trunc)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of word index: 116055\n",
            "Saving the word index as JSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSHwIoCLPKPq"
      },
      "source": [
        "# Convert to Numpy arrays, so as to get it to work with TensorFlow 2.x\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_lbls_text)\n",
        "validation_padded = np.array(validation_padded)\n",
        "validation_labels = np.array(validation_lbls_text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OLtjmJw9ehH"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5ADD9CuNFWB",
        "outputId": "578f6885-ee3c-4388-ae5f-be79be8c84d0"
      },
      "source": [
        "# Callbacks to cancel training after reaching a desired accuracy\n",
        "# This is done to avoid overfitting\n",
        "DESIRED_ACCURACY = 0.999\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if logs.get('accuracy') > DESIRED_ACCURACY:\n",
        "      print(\"Reached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "# Sequential - defines a SEQUENCE of layers in the neural network.\n",
        "model = tf.keras.Sequential([\n",
        "    # Embedding - Turns positive integers (indexes) into dense vectors of fixed size (here dimensions = 32).\n",
        "    tf.keras.layers.Embedding(size_vocab, dimensions, input_length=length_text),\n",
        "    # 1D convolution layer - filter size = 128, convolution window = 5, activation fn = ReLU\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "    # Global average pooling operation (Flattening)\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    # Regular densely-connected Neural Network layer with ReLU activation function.\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    # Regular densely-connected Neural Network layer with sigmoid activation function.\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# model.compile - Configures the model for training.\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "# Adam -  optimization algorithm used instead of the classical stochastic gradient descent procedure to update network weights.\n",
        "\n",
        "# Display the summary of the model\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 60, 32)            96000     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 56, 64)            10304     \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 64)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 24)                1560      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 107,889\n",
            "Trainable params: 107,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb2NlD159g8k"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TP5ThxvNPK8",
        "outputId": "a8a0ce93-c24f-4067-d296-242224b07fff"
      },
      "source": [
        "num_epochs = 15\n",
        "\n",
        "# model.fit - Train the model for a fixed number of epochs\n",
        "history = model.fit(training_padded, \n",
        "                    training_labels, \n",
        "                    epochs=num_epochs, \n",
        "                    validation_data=(\n",
        "                        validation_padded, \n",
        "                        validation_labels), \n",
        "                    verbose=1)\n",
        "                    #callbacks=[callbacks])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "782/782 [==============================] - 6s 6ms/step - loss: 0.0490 - accuracy: 0.9928 - val_loss: 0.0040 - val_accuracy: 0.9991\n",
            "Epoch 2/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0031 - val_accuracy: 0.9991\n",
            "Epoch 3/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 6.8782e-04 - accuracy: 0.9998 - val_loss: 0.0015 - val_accuracy: 0.9995\n",
            "Epoch 4/15\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 3.7045e-04 - accuracy: 0.9999 - val_loss: 0.0017 - val_accuracy: 0.9995\n",
            "Epoch 5/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.6924e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 0.9995\n",
            "Epoch 6/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.4431e-04 - accuracy: 1.0000 - val_loss: 4.5207e-04 - val_accuracy: 0.9998\n",
            "Epoch 7/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.0030e-04 - accuracy: 1.0000 - val_loss: 5.9146e-04 - val_accuracy: 0.9998\n",
            "Epoch 8/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.4301e-04 - accuracy: 1.0000 - val_loss: 4.5290e-04 - val_accuracy: 0.9998\n",
            "Epoch 9/15\n",
            "782/782 [==============================] - 4s 6ms/step - loss: 1.0017e-04 - accuracy: 1.0000 - val_loss: 1.6199e-04 - val_accuracy: 0.9998\n",
            "Epoch 10/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 4.4268e-05 - accuracy: 1.0000 - val_loss: 5.6570e-05 - val_accuracy: 1.0000\n",
            "Epoch 11/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 4.8008e-06 - accuracy: 1.0000 - val_loss: 6.4852e-05 - val_accuracy: 1.0000\n",
            "Epoch 12/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 2.9102e-06 - accuracy: 1.0000 - val_loss: 5.8143e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.9035e-06 - accuracy: 1.0000 - val_loss: 5.5504e-05 - val_accuracy: 1.0000\n",
            "Epoch 14/15\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 1.5203e-06 - accuracy: 1.0000 - val_loss: 3.3928e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/15\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 9.2568e-07 - accuracy: 1.0000 - val_loss: 2.0561e-05 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poN5h1SD9tiE"
      },
      "source": [
        "## Plotting Accuracy and Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCDbITWsNWlI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the accuracy and loss functions\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9ymTQswYUQz"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVdCGN-tYXqX"
      },
      "source": [
        "import seaborn\n",
        "print('Confusion Matrix')\n",
        "y_predicted = model.predict(validation_padded)\n",
        "y_predicted_labels = y_predicted > 0.5\n",
        "\n",
        "size = np.size(y_predicted_labels)\n",
        "y_predicted_labels = y_predicted_labels.reshape(size, )\n",
        "\n",
        "for i in range (1, 5):\n",
        "  total = i * size // 4\n",
        "  cm = tf.math.confusion_matrix(labels=validation_labels[0:total],predictions=y_predicted_labels[0:total])\n",
        "\n",
        "  # Calculate accuracy\n",
        "  cm_np = cm.numpy()\n",
        "  conf_acc = (cm_np[0, 0] + cm_np[1, 1])/ np.sum(cm_np) * 100\n",
        "  print(\"Accuracy for\", str(total), \"Test Data = \", conf_acc)\n",
        "\n",
        "  # Plot the confusion matrix\n",
        "  plt.figure(figsize = (10,7))\n",
        "  seaborn.heatmap(cm, annot=True, fmt='d')\n",
        "  plt.title(\"Confusion Matrix for \" + str(total) + \" Test Data\")\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Expected')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f94fRNNo-386"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_rBTIVTQHow"
      },
      "source": [
        "# Save and convert the model (Used for deploying in web application)\n",
        "model.save('model/text_model.h5')\n",
        "print(\"Saved the model successfully\")\n",
        "\n",
        "!apt-get -qq install virtualenv\n",
        "!virtualenv -p python3 venv\n",
        "!source venv/bin/activate\n",
        "!pip install -q tensorflowjs\n",
        "!tensorflowjs_converter --input_format=keras /content/model/text_model.h5 /content/text_model\n",
        "print(\"Model converted to JSON successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plgECuQc9wdt"
      },
      "source": [
        "## Sample Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyfb-NtpNV7h"
      },
      "source": [
        "# Sample examples\n",
        "sentence = [\"My credit card no is 124345346\", \"game of thrones season finale showing this sunday night\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=length_text, padding=padding_type, truncating=type_trunc)\n",
        "predictions = model.predict(padded)\n",
        "print(\"OUPUT for text model\")\n",
        "for i in range(len(predictions)):\n",
        "  print(predictions[i][0])\n",
        "  if predictions[i][0]>0.5:\n",
        "    print(\"Sensitive - \"+ sentence[i])\n",
        "  else:\n",
        "    print(\"Non-Sensitive - \" + sentence[i] )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}